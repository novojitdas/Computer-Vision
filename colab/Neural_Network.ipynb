{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhdPKUczYWNhLJcEErfnEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novojitdas/Computer-Vision/blob/main/colab/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network"
      ],
      "metadata": {
        "id": "uvSSCU4ZM--E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![nn](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/perceptron-768x559.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "##Activation Functions\n",
        "\n",
        " Activation functions allowing it to learn complex patterns and representations from the data. Activation functions determine the output of a neuron or node in a neural network based on its weighted input.\n",
        "\n",
        " **Sigmoid Function (Logistic Activation):** The sigmoid function squashes the input values to a range between 0 and 1. It's often used in binary classification problems, where the output represents probabilities.\n",
        "\n",
        "**Hyperbolic Tangent Function (tanh):** The tanh function is similar to the sigmoid but maps input values to a range between -1 and 1, making it zero-centered. It is often used in hidden layers of neural networks.\n",
        "\n",
        "**Rectified Linear Unit (ReLU):** ReLU is one of the most widely used activation functions. It returns the input for positive values and zero for negative values. It's computationally efficient and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "**Leaky ReLU:** Leaky ReLU is a variation of ReLU that allows a small gradient for negative inputs, which helps mitigate the \"dying ReLU\" problem where neurons can become inactive during training.\n",
        "\n",
        "**Exponential Linear Unit (ELU):** ELU is another variation of ReLU that has a smoother transition for negative values, which can help with training deep networks. It also allows negative values.\n",
        "\n",
        "![img](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/activation_functions-768x585.png?lossy=2&strip=1&webp=1)"
      ],
      "metadata": {
        "id": "HQEpGTxsdk8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perceptron\n",
        "\n",
        "A perceptron is a fundamental building block in neural networks, and it serves as the simplest form of an artificial neuron. It was developed by Frank Rosenblatt in the late 1950s. A perceptron takes a set of inputs, performs a weighted sum of those inputs, and then applies an activation function to produce an output. The output is often binary (0 or 1) or can be a continuous value, depending on the activation function used.\n",
        "\n",
        "![perceptron](https://i.imgur.com/z0cHJoX.png)\n",
        "\n",
        "Here are the key components and characteristics of a perceptron in a neural network:\n",
        "\n",
        "Input: A perceptron receives a set of input values, often denoted as x1, x2, x3, ..., xn. These inputs can represent features, and they can be real numbers or binary values.\n",
        "\n",
        "Weights: Each input is associated with a weight (often denoted as w1, w2, w3, ..., wn). These weights represent the strength of the connection between the inputs and the perceptron.\n",
        "\n",
        "Weighted Sum: The perceptron computes a weighted sum of the inputs and weights. This sum is represented as:\n",
        "\n",
        "Weighted Sum = (w1 * x1) + (w2 * x2) + (w3 * x3) + ... + (wn * xn)\n",
        "\n",
        "Bias: In addition to the weighted sum, a bias term (often denoted as b) is added to the sum. The bias allows the perceptron to account for situations where all inputs are zero, and it helps in controlling the activation threshold of the neuron.\n",
        "\n",
        "Activation Function: The weighted sum plus the bias is then passed through an activation function (sometimes called a transfer function). The activation function determines the output of the perceptron. Common activation functions include the step function (a simple binary decision), the sigmoid function, the hyperbolic tangent (tanh), or the rectified linear unit (ReLU) function, among others.\n",
        "\n",
        "The step function: Output is 1 if the weighted sum + bias is greater than or equal to 0, and 0 otherwise.\n",
        "Sigmoid function: Produces an output in the range (0, 1), making it suitable for binary classification tasks.\n",
        "Hyperbolic tangent (tanh): Produces an output in the range (-1, 1).\n",
        "ReLU function: Output is zero if the weighted sum + bias is less than 0, and it is equal to the weighted sum + bias otherwise.\n",
        "Output: The final output of the perceptron is the result of the activation function. This output is used as an input for subsequent layers in a neural network.\n",
        "\n",
        "Perceptrons are limited in their capabilities and can only model linearly separable functions, which means they can't handle more complex, nonlinear problems. However, by combining multiple perceptrons in layers and using more advanced activation functions, neural networks can approximate a wide range of functions, making them suitable for tasks like image recognition, natural language processing, and many other machine learning and artificial intelligence applications. These more complex neural networks are often referred to as multi-layer perceptrons (MLPs) or feedforward neural networks."
      ],
      "metadata": {
        "id": "xkSykvRfo9f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Packages\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zW0blOKSsHjm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Perceptron"
      ],
      "metadata": {
        "id": "Aj_yRMIysfSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "\tdef __init__(self, N, alpha=0.1):\n",
        "\t\t# initialize the weight matrix and store the learning rate\n",
        "\t\tself.W = np.random.randn(N + 1) / np.sqrt(N)\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\tdef step(self, x):\n",
        "\t\t# apply the step function\n",
        "\t\treturn 1 if x > 0 else 0\n",
        "\n",
        "\tdef fit(self, X, y, epochs=10):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# loop over the desired number of epochs\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\t# take the dot product between the input features\n",
        "\t\t\t\t# and the weight matrix, then pass this value\n",
        "\t\t\t\t# through the step function to obtain the prediction\n",
        "\t\t\t\tp = self.step(np.dot(x, self.W))\n",
        "\n",
        "\t\t\t\t# only perform a weight update if our prediction\n",
        "\t\t\t\t# does not match the target\n",
        "\t\t\t\tif p != target:\n",
        "\t\t\t\t\t# determine the error\n",
        "\t\t\t\t\terror = p - target\n",
        "\n",
        "\t\t\t\t\t# update the weight matrix\n",
        "\t\t\t\t\tself.W += -self.alpha * error * x\n",
        "\n",
        "\tdef predict(self, X, addBias=True):\n",
        "\t\t# ensure our input is a matrix\n",
        "\t\tX = np.atleast_2d(X)\n",
        "\n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# take the dot product between the input features and the\n",
        "\t\t# weight matrix, then pass the value through the step\n",
        "\t\t# function\n",
        "\t\treturn self.step(np.dot(X, self.W))"
      ],
      "metadata": {
        "id": "J7jDUSEisZ-E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron on bitwise datasets"
      ],
      "metadata": {
        "id": "ukhD8gOvsy9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OR\n"
      ],
      "metadata": {
        "id": "yGPQiphjtJ8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the OR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPSQi27EtXf0",
        "outputId": "fc9b6456-c2e0-42d8-c567-1e353ab884f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awvU3instr7m",
        "outputId": "0e17aa49-6312-4a20-be0f-ea573c990446"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0\n",
            "[INFO] data=[0 1], ground-truth=1, pred=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=1\n",
            "[INFO] data=[1 1], ground-truth=1, pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AND"
      ],
      "metadata": {
        "id": "tAx21PymtOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the AND dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)\n",
        "\n",
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjfZdSKxtLv8",
        "outputId": "f4fb6880-07d3-4484-dbb6-697a75c3d6cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n",
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0\n",
            "[INFO] data=[0 1], ground-truth=0, pred=0\n",
            "[INFO] data=[1 0], ground-truth=0, pred=0\n",
            "[INFO] data=[1 1], ground-truth=1, pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###XOR"
      ],
      "metadata": {
        "id": "nCrF_FiOtQW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)\n",
        "\n",
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPteJVuJtSE8",
        "outputId": "b5e5f9fa-6063-4c70-b04a-2a22b149576e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n",
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=1\n",
            "[INFO] data=[0 1], ground-truth=1, pred=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=0\n",
            "[INFO] data=[1 1], ground-truth=0, pred=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Backpropagation\n",
        "The backpropagation algorithm consists of two phases:\n",
        "\n",
        "1. **The forward pass:** where our inputs are passed through the network and output predictions obtained (also known as the propagation phase).\n",
        "\n",
        "2. **The backward pass:** where we compute the gradient of the loss function at the final layer (i.e., predictions layer) of the network and use this gradient to recursively apply the chain rule to update the weights in our network (also known as the weight update phase).\n",
        "\n",
        "To obtain perfect classification accuracy on this problem we’ll need a feedforward neural network with at least a single hidden layer, so let’s go ahead and start with a 2−2−1 architecture (Figure 1, top). This is a good start; however, we’re forgetting to include the bias term. There are two ways to include the bias term b in our network. We can either:\n",
        "\n",
        "1. Use a separate variable.\n",
        "2. Treat the bias as a trainable parameter within the weight matrix by inserting a column of 1’s into the feature vectors.\n",
        "\n",
        "![arch](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/forward_pass_setup-768x774.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "Inserting a column of 1’s into our feature vector is done programmatically, but to ensure we understand this point, let’s update our XOR design matrix to explicitly see this taking place (Table 1, right). As you can see, a column of 1’s have been added to our feature vectors. In practice you can insert this column anywhere you like, but we typically place it either as (1) the first entry in the feature vector or (2) the last entry in the feature vector.\n",
        "\n",
        "Since we have changed the size of our input feature vector (normally performed inside neural network implementation itself so that we do not need to explicitly modify our design matrix), that changes our (perceived) network architecture from 2−2−1 to an (internal) 3−3−1 (Figure 1, bottom).\n",
        "\n",
        "We’ll still refer to this network architecture as 2−2−1, but when it comes to implementation, it’s actually 3−3−1 due to the addition of the bias term embedded in the weight matrix.\n",
        "\n",
        "Finally, recall that both our input layer and all hidden layers require a bias term; however, the final output layer does not require a bias. The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer — it is now a trainable parameter within the weight matrix, thus making training more efficient and substantially easier to implement.\n",
        "\n",
        "To see the forward pass in action, we first initialize the weights in our network, as in Figure 2. Notice how each arrow in the weight matrix has a value associated with it — this is the current weight value for a given node and signifies the amount in which a given input is amplified or diminished. This weight value will then be updated during the backpropagation phase.\n",
        "\n",
        "![fig2](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/forward_pass_init-768x616.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "Inserting a column of 1’s into our feature vector is done programmatically, but to ensure we understand this point, let’s update our XOR design matrix to explicitly see this taking place (Table 1, right). As you can see, a column of 1’s have been added to our feature vectors. In practice you can insert this column anywhere you like, but we typically place it either as (1) the first entry in the feature vector or (2) the last entry in the feature vector.\n",
        "\n",
        "Since we have changed the size of our input feature vector (normally performed inside neural network implementation itself so that we do not need to explicitly modify our design matrix), that changes our (perceived) network architecture from 2−2−1 to an (internal) 3−3−1 (Figure 1, bottom).\n",
        "\n",
        "We’ll still refer to this network architecture as 2−2−1, but when it comes to implementation, it’s actually 3−3−1 due to the addition of the bias term embedded in the weight matrix.\n",
        "\n",
        "Finally, recall that both our input layer and all hidden layers require a bias term; however, the final output layer does not require a bias. The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer — it is now a trainable parameter within the weight matrix, thus making training more efficient and substantially easier to implement.\n",
        "\n",
        "To see the forward pass in action, we first initialize the weights in our network, as in Figure 2. Notice how each arrow in the weight matrix has a value associated with it — this is the current weight value for a given node and signifies the amount in which a given input is amplified or diminished. This weight value will then be updated during the backpropagation phase.\n",
        "\n",
        "On the far left of Figure 2, we present the feature vector (0, 1, 1) (and target output value 1 to the network). Here we can see that 0, 1, and 1 have been assigned to the three input nodes in the network. To propagate the values through the network and obtain the final classification, we need to take the dot product between the inputs and the weight values, followed by applying an activation function (in this case, the sigmoid function, σ).\n",
        "\n",
        "Let’s compute the inputs to the three nodes in the hidden layers:\n",
        "\n",
        "σ((0×0.351) + (1×1.076) + (1×1.116)) = 0.899\n",
        "σ((0× −0.097) + (1× −0.165) + (1×0.542)) = 0.593\n",
        "σ((0×0.457) + (1× −0.165) + (1× −0.331)) = 0.378\n",
        "Looking at the node values of the hidden layers (Figure 2, middle), we can see the nodes have been updated to reflect our computation.\n",
        "\n",
        "We now have our inputs to the hidden layer nodes. To compute the output prediction, we once again compute the dot product followed by a sigmoid activation:\n",
        "\n",
        "(1) σ((0.899×0.383) + (0.593× −0.327) + (0.378× −0.329)) = 0.506\n",
        "\n",
        "The output of the network is thus 0.506. We can apply a step function to determine if this output is the correct classification or not:\n",
        "\n",
        "![formula](https://b2633864.smushcdn.com/2633864/wp-content/latex/f4d/f4d4c9bedaba059ad269625523526821-ffffff-000000-0.png?size=172x51&lossy=2&strip=1&webp=1)\n",
        "\n",
        "Applying the step function with net = 0.506 we see that our network predicts 1 which is, in fact, the correct class label. However, our network is not very confident in this class label — the predicted value 0.506 is very close to the threshold of the step. Ideally, this prediction should be closer to 0.98−0.99, implying that our network has truly learned the underlying pattern in the dataset. In order for our network to actually “learn,” we need to apply the backward pass."
      ],
      "metadata": {
        "id": "09-GbVvoNIxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Impementing Backpropagation"
      ],
      "metadata": {
        "id": "96DivCLjO9qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YykIDTmKPCgG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "\tdef __init__(self, layers, alpha=0.1):\n",
        "\t\t# initialize the list of weights matrices, then store the\n",
        "\t\t# network architecture and learning rate\n",
        "\t\tself.W = []\n",
        "\t\tself.layers = layers\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\t\t# start looping from the index of the first layer but\n",
        "\t\t# stop before we reach the last two layers\n",
        "\t\tfor i in np.arange(0, len(layers) - 2):\n",
        "\t\t\t# randomly initialize a weight matrix connecting the\n",
        "\t\t\t# number of nodes in each respective layer together,\n",
        "\t\t\t# adding an extra node for the bias\n",
        "\t\t\tw = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "\t\t\tself.W.append(w / np.sqrt(layers[i]))\n",
        "\n",
        "\t\t# the last two layers are a special case where the input\n",
        "\t\t# connections need a bias term but the output does not\n",
        "\t\tw = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "\t\tself.W.append(w / np.sqrt(layers[-2]))\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\t# construct and return a string that represents the network\n",
        "\t\t# architecture\n",
        "\t\treturn \"NeuralNetwork: {}\".format(\n",
        "\t\t\t\"-\".join(str(l) for l in self.layers))\n",
        "\n",
        "\tdef sigmoid(self, x):\n",
        "\t\t# compute and return the sigmoid activation value for a\n",
        "\t\t# given input value\n",
        "\t\treturn 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "\tdef sigmoid_deriv(self, x):\n",
        "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
        "\t\t# that `x` has already been passed through the `sigmoid`\n",
        "\t\t# function\n",
        "\t\treturn x * (1 - x)\n",
        "\n",
        "\tdef fit(self, X, y, epochs=1000, displayUpdate=100):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# loop over the desired number of epochs\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point and train\n",
        "\t\t\t# our network on it\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\tself.fit_partial(x, target)\n",
        "\n",
        "\t\t\t# check to see if we should display a training update\n",
        "\t\t\tif epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
        "\t\t\t\tloss = self.calculate_loss(X, y)\n",
        "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
        "\t\t\t\t\tepoch + 1, loss))\n",
        "\n",
        "\tdef fit_partial(self, x, y):\n",
        "\t\t# construct our list of output activations for each layer\n",
        "\t\t# as our data point flows through the network; the first\n",
        "\t\t# activation is a special case -- it's just the input\n",
        "\t\t# feature vector itself\n",
        "\t\tA = [np.atleast_2d(x)]\n",
        "\n",
        "\t\t# FEEDFORWARD:\n",
        "\t\t# loop over the layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# feedforward the activation at the current layer by\n",
        "\t\t\t# taking the dot product between the activation and\n",
        "\t\t\t# the weight matrix -- this is called the \"net input\"\n",
        "\t\t\t# to the current layer\n",
        "\t\t\tnet = A[layer].dot(self.W[layer])\n",
        "\n",
        "\t\t\t# computing the \"net output\" is simply applying our\n",
        "\t\t\t# non-linear activation function to the net input\n",
        "\t\t\tout = self.sigmoid(net)\n",
        "\n",
        "\t\t\t# once we have the net output, add it to our list of\n",
        "\t\t\t# activations\n",
        "\t\t\tA.append(out)\n",
        "\n",
        "\t\t# BACKPROPAGATION\n",
        "\t\t# the first phase of backpropagation is to compute the\n",
        "\t\t# difference between our *prediction* (the final output\n",
        "\t\t# activation in the activations list) and the true target\n",
        "\t\t# value\n",
        "\t\terror = A[-1] - y\n",
        "\n",
        "\t\t# from here, we need to apply the chain rule and build our\n",
        "\t\t# list of deltas `D`; the first entry in the deltas is\n",
        "\t\t# simply the error of the output layer times the derivative\n",
        "\t\t# of our activation function for the output value\n",
        "\t\tD = [error * self.sigmoid_deriv(A[-1])]\n",
        "\n",
        "\t\t# once you understand the chain rule it becomes super easy\n",
        "\t\t# to implement with a `for` loop -- simply loop over the\n",
        "\t\t# layers in reverse order (ignoring the last two since we\n",
        "\t\t# already have taken them into account)\n",
        "\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n",
        "\t\t\t# the delta for the current layer is equal to the delta\n",
        "\t\t\t# of the *previous layer* dotted with the weight matrix\n",
        "\t\t\t# of the current layer, followed by multiplying the delta\n",
        "\t\t\t# by the derivative of the non-linear activation function\n",
        "\t\t\t# for the activations of the current layer\n",
        "\t\t\tdelta = D[-1].dot(self.W[layer].T)\n",
        "\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n",
        "\t\t\tD.append(delta)\n",
        "\n",
        "\t\t# since we looped over our layers in reverse order we need to\n",
        "\t\t# reverse the deltas\n",
        "\t\tD = D[::-1]\n",
        "\n",
        "\t\t# WEIGHT UPDATE PHASE\n",
        "\t\t# loop over the layers\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# update our weights by taking the dot product of the layer\n",
        "\t\t\t# activations with their respective deltas, then multiplying\n",
        "\t\t\t# this value by some small learning rate and adding to our\n",
        "\t\t\t# weight matrix -- this is where the actual \"learning\" takes\n",
        "\t\t\t# place\n",
        "\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "\n",
        "\tdef predict(self, X, addBias=True):\n",
        "\t\t# initialize the output prediction as the input features -- this\n",
        "\t\t# value will be (forward) propagated through the network to\n",
        "\t\t# obtain the final prediction\n",
        "\t\tp = np.atleast_2d(X)\n",
        "\n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
        "\n",
        "\t\t# loop over our layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# computing the output prediction is as simple as taking\n",
        "\t\t\t# the dot product between the current activation value `p`\n",
        "\t\t\t# and the weight matrix associated with the current layer,\n",
        "\t\t\t# then passing this value through a non-linear activation\n",
        "\t\t\t# function\n",
        "\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "\n",
        "\t\t# return the predicted value\n",
        "\t\treturn p\n",
        "\n",
        "\tdef calculate_loss(self, X, targets):\n",
        "\t\t# make predictions for the input data points then compute\n",
        "\t\t# the loss\n",
        "\t\ttargets = np.atleast_2d(targets)\n",
        "\t\tpredictions = self.predict(X, addBias=False)\n",
        "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\n",
        "\t\t# return the loss\n",
        "\t\treturn loss"
      ],
      "metadata": {
        "id": "R8BJ13wyPJ0A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backpropagation with bitwise xor"
      ],
      "metadata": {
        "id": "-iVHVQkQPZjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# define our 2-2-1 neural network and train it\n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000)\n",
        "\n",
        "# now that our network is trained, loop over the XOR data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = nn.predict(x)[0][0]\n",
        "\tstep = 1 if pred > 0.5 else 0\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
        "\t\tx, target[0], pred, step))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDFb_KKjPeG4",
        "outputId": "72ac68d9-10c2-4df7-b635-9a397d97d838"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch=1, loss=0.5011958\n",
            "[INFO] epoch=100, loss=0.4993236\n",
            "[INFO] epoch=200, loss=0.4986790\n",
            "[INFO] epoch=300, loss=0.4973990\n",
            "[INFO] epoch=400, loss=0.4942430\n",
            "[INFO] epoch=500, loss=0.4852055\n",
            "[INFO] epoch=600, loss=0.4609070\n",
            "[INFO] epoch=700, loss=0.4127845\n",
            "[INFO] epoch=800, loss=0.3217109\n",
            "[INFO] epoch=900, loss=0.1589114\n",
            "[INFO] epoch=1000, loss=0.0645025\n",
            "[INFO] epoch=1100, loss=0.0337388\n",
            "[INFO] epoch=1200, loss=0.0213986\n",
            "[INFO] epoch=1300, loss=0.0152215\n",
            "[INFO] epoch=1400, loss=0.0116346\n",
            "[INFO] epoch=1500, loss=0.0093325\n",
            "[INFO] epoch=1600, loss=0.0077466\n",
            "[INFO] epoch=1700, loss=0.0065959\n",
            "[INFO] epoch=1800, loss=0.0057269\n",
            "[INFO] epoch=1900, loss=0.0050498\n",
            "[INFO] epoch=2000, loss=0.0045088\n",
            "[INFO] epoch=2100, loss=0.0040675\n",
            "[INFO] epoch=2200, loss=0.0037012\n",
            "[INFO] epoch=2300, loss=0.0033928\n",
            "[INFO] epoch=2400, loss=0.0031297\n",
            "[INFO] epoch=2500, loss=0.0029028\n",
            "[INFO] epoch=2600, loss=0.0027053\n",
            "[INFO] epoch=2700, loss=0.0025320\n",
            "[INFO] epoch=2800, loss=0.0023787\n",
            "[INFO] epoch=2900, loss=0.0022422\n",
            "[INFO] epoch=3000, loss=0.0021200\n",
            "[INFO] epoch=3100, loss=0.0020100\n",
            "[INFO] epoch=3200, loss=0.0019104\n",
            "[INFO] epoch=3300, loss=0.0018198\n",
            "[INFO] epoch=3400, loss=0.0017372\n",
            "[INFO] epoch=3500, loss=0.0016615\n",
            "[INFO] epoch=3600, loss=0.0015919\n",
            "[INFO] epoch=3700, loss=0.0015277\n",
            "[INFO] epoch=3800, loss=0.0014683\n",
            "[INFO] epoch=3900, loss=0.0014132\n",
            "[INFO] epoch=4000, loss=0.0013620\n",
            "[INFO] epoch=4100, loss=0.0013142\n",
            "[INFO] epoch=4200, loss=0.0012696\n",
            "[INFO] epoch=4300, loss=0.0012278\n",
            "[INFO] epoch=4400, loss=0.0011886\n",
            "[INFO] epoch=4500, loss=0.0011517\n",
            "[INFO] epoch=4600, loss=0.0011170\n",
            "[INFO] epoch=4700, loss=0.0010843\n",
            "[INFO] epoch=4800, loss=0.0010534\n",
            "[INFO] epoch=4900, loss=0.0010241\n",
            "[INFO] epoch=5000, loss=0.0009964\n",
            "[INFO] epoch=5100, loss=0.0009701\n",
            "[INFO] epoch=5200, loss=0.0009451\n",
            "[INFO] epoch=5300, loss=0.0009213\n",
            "[INFO] epoch=5400, loss=0.0008987\n",
            "[INFO] epoch=5500, loss=0.0008771\n",
            "[INFO] epoch=5600, loss=0.0008565\n",
            "[INFO] epoch=5700, loss=0.0008368\n",
            "[INFO] epoch=5800, loss=0.0008180\n",
            "[INFO] epoch=5900, loss=0.0008000\n",
            "[INFO] epoch=6000, loss=0.0007828\n",
            "[INFO] epoch=6100, loss=0.0007662\n",
            "[INFO] epoch=6200, loss=0.0007503\n",
            "[INFO] epoch=6300, loss=0.0007351\n",
            "[INFO] epoch=6400, loss=0.0007204\n",
            "[INFO] epoch=6500, loss=0.0007063\n",
            "[INFO] epoch=6600, loss=0.0006927\n",
            "[INFO] epoch=6700, loss=0.0006797\n",
            "[INFO] epoch=6800, loss=0.0006671\n",
            "[INFO] epoch=6900, loss=0.0006549\n",
            "[INFO] epoch=7000, loss=0.0006432\n",
            "[INFO] epoch=7100, loss=0.0006319\n",
            "[INFO] epoch=7200, loss=0.0006209\n",
            "[INFO] epoch=7300, loss=0.0006103\n",
            "[INFO] epoch=7400, loss=0.0006001\n",
            "[INFO] epoch=7500, loss=0.0005902\n",
            "[INFO] epoch=7600, loss=0.0005806\n",
            "[INFO] epoch=7700, loss=0.0005713\n",
            "[INFO] epoch=7800, loss=0.0005623\n",
            "[INFO] epoch=7900, loss=0.0005536\n",
            "[INFO] epoch=8000, loss=0.0005451\n",
            "[INFO] epoch=8100, loss=0.0005369\n",
            "[INFO] epoch=8200, loss=0.0005289\n",
            "[INFO] epoch=8300, loss=0.0005211\n",
            "[INFO] epoch=8400, loss=0.0005136\n",
            "[INFO] epoch=8500, loss=0.0005063\n",
            "[INFO] epoch=8600, loss=0.0004991\n",
            "[INFO] epoch=8700, loss=0.0004922\n",
            "[INFO] epoch=8800, loss=0.0004855\n",
            "[INFO] epoch=8900, loss=0.0004789\n",
            "[INFO] epoch=9000, loss=0.0004725\n",
            "[INFO] epoch=9100, loss=0.0004663\n",
            "[INFO] epoch=9200, loss=0.0004602\n",
            "[INFO] epoch=9300, loss=0.0004543\n",
            "[INFO] epoch=9400, loss=0.0004485\n",
            "[INFO] epoch=9500, loss=0.0004429\n",
            "[INFO] epoch=9600, loss=0.0004374\n",
            "[INFO] epoch=9700, loss=0.0004320\n",
            "[INFO] epoch=9800, loss=0.0004268\n",
            "[INFO] epoch=9900, loss=0.0004217\n",
            "[INFO] epoch=10000, loss=0.0004167\n",
            "[INFO] epoch=10100, loss=0.0004118\n",
            "[INFO] epoch=10200, loss=0.0004070\n",
            "[INFO] epoch=10300, loss=0.0004024\n",
            "[INFO] epoch=10400, loss=0.0003978\n",
            "[INFO] epoch=10500, loss=0.0003933\n",
            "[INFO] epoch=10600, loss=0.0003890\n",
            "[INFO] epoch=10700, loss=0.0003847\n",
            "[INFO] epoch=10800, loss=0.0003805\n",
            "[INFO] epoch=10900, loss=0.0003765\n",
            "[INFO] epoch=11000, loss=0.0003724\n",
            "[INFO] epoch=11100, loss=0.0003685\n",
            "[INFO] epoch=11200, loss=0.0003647\n",
            "[INFO] epoch=11300, loss=0.0003609\n",
            "[INFO] epoch=11400, loss=0.0003572\n",
            "[INFO] epoch=11500, loss=0.0003536\n",
            "[INFO] epoch=11600, loss=0.0003501\n",
            "[INFO] epoch=11700, loss=0.0003466\n",
            "[INFO] epoch=11800, loss=0.0003432\n",
            "[INFO] epoch=11900, loss=0.0003398\n",
            "[INFO] epoch=12000, loss=0.0003366\n",
            "[INFO] epoch=12100, loss=0.0003333\n",
            "[INFO] epoch=12200, loss=0.0003302\n",
            "[INFO] epoch=12300, loss=0.0003271\n",
            "[INFO] epoch=12400, loss=0.0003240\n",
            "[INFO] epoch=12500, loss=0.0003211\n",
            "[INFO] epoch=12600, loss=0.0003181\n",
            "[INFO] epoch=12700, loss=0.0003152\n",
            "[INFO] epoch=12800, loss=0.0003124\n",
            "[INFO] epoch=12900, loss=0.0003096\n",
            "[INFO] epoch=13000, loss=0.0003069\n",
            "[INFO] epoch=13100, loss=0.0003042\n",
            "[INFO] epoch=13200, loss=0.0003016\n",
            "[INFO] epoch=13300, loss=0.0002990\n",
            "[INFO] epoch=13400, loss=0.0002964\n",
            "[INFO] epoch=13500, loss=0.0002939\n",
            "[INFO] epoch=13600, loss=0.0002914\n",
            "[INFO] epoch=13700, loss=0.0002890\n",
            "[INFO] epoch=13800, loss=0.0002866\n",
            "[INFO] epoch=13900, loss=0.0002843\n",
            "[INFO] epoch=14000, loss=0.0002819\n",
            "[INFO] epoch=14100, loss=0.0002797\n",
            "[INFO] epoch=14200, loss=0.0002774\n",
            "[INFO] epoch=14300, loss=0.0002752\n",
            "[INFO] epoch=14400, loss=0.0002730\n",
            "[INFO] epoch=14500, loss=0.0002709\n",
            "[INFO] epoch=14600, loss=0.0002688\n",
            "[INFO] epoch=14700, loss=0.0002667\n",
            "[INFO] epoch=14800, loss=0.0002647\n",
            "[INFO] epoch=14900, loss=0.0002627\n",
            "[INFO] epoch=15000, loss=0.0002607\n",
            "[INFO] epoch=15100, loss=0.0002587\n",
            "[INFO] epoch=15200, loss=0.0002568\n",
            "[INFO] epoch=15300, loss=0.0002549\n",
            "[INFO] epoch=15400, loss=0.0002530\n",
            "[INFO] epoch=15500, loss=0.0002512\n",
            "[INFO] epoch=15600, loss=0.0002494\n",
            "[INFO] epoch=15700, loss=0.0002476\n",
            "[INFO] epoch=15800, loss=0.0002458\n",
            "[INFO] epoch=15900, loss=0.0002441\n",
            "[INFO] epoch=16000, loss=0.0002424\n",
            "[INFO] epoch=16100, loss=0.0002407\n",
            "[INFO] epoch=16200, loss=0.0002390\n",
            "[INFO] epoch=16300, loss=0.0002373\n",
            "[INFO] epoch=16400, loss=0.0002357\n",
            "[INFO] epoch=16500, loss=0.0002341\n",
            "[INFO] epoch=16600, loss=0.0002325\n",
            "[INFO] epoch=16700, loss=0.0002310\n",
            "[INFO] epoch=16800, loss=0.0002294\n",
            "[INFO] epoch=16900, loss=0.0002279\n",
            "[INFO] epoch=17000, loss=0.0002264\n",
            "[INFO] epoch=17100, loss=0.0002249\n",
            "[INFO] epoch=17200, loss=0.0002235\n",
            "[INFO] epoch=17300, loss=0.0002220\n",
            "[INFO] epoch=17400, loss=0.0002206\n",
            "[INFO] epoch=17500, loss=0.0002192\n",
            "[INFO] epoch=17600, loss=0.0002178\n",
            "[INFO] epoch=17700, loss=0.0002164\n",
            "[INFO] epoch=17800, loss=0.0002151\n",
            "[INFO] epoch=17900, loss=0.0002137\n",
            "[INFO] epoch=18000, loss=0.0002124\n",
            "[INFO] epoch=18100, loss=0.0002111\n",
            "[INFO] epoch=18200, loss=0.0002098\n",
            "[INFO] epoch=18300, loss=0.0002085\n",
            "[INFO] epoch=18400, loss=0.0002073\n",
            "[INFO] epoch=18500, loss=0.0002060\n",
            "[INFO] epoch=18600, loss=0.0002048\n",
            "[INFO] epoch=18700, loss=0.0002036\n",
            "[INFO] epoch=18800, loss=0.0002024\n",
            "[INFO] epoch=18900, loss=0.0002012\n",
            "[INFO] epoch=19000, loss=0.0002000\n",
            "[INFO] epoch=19100, loss=0.0001988\n",
            "[INFO] epoch=19200, loss=0.0001977\n",
            "[INFO] epoch=19300, loss=0.0001965\n",
            "[INFO] epoch=19400, loss=0.0001954\n",
            "[INFO] epoch=19500, loss=0.0001943\n",
            "[INFO] epoch=19600, loss=0.0001932\n",
            "[INFO] epoch=19700, loss=0.0001921\n",
            "[INFO] epoch=19800, loss=0.0001911\n",
            "[INFO] epoch=19900, loss=0.0001900\n",
            "[INFO] epoch=20000, loss=0.0001889\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0.0102, step=0\n",
            "[INFO] data=[0 1], ground-truth=1, pred=0.9908, step=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=0.9894, step=1\n",
            "[INFO] data=[1 1], ground-truth=0, pred=0.0087, step=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MNIST"
      ],
      "metadata": {
        "id": "eOLm9WHHRYzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST dataset and apply min/max scaling to scale the\n",
        "# pixel intensity values to the range [0, 1] (each image is\n",
        "# represented by an 8 x 8 = 64-dim feature vector)\n",
        "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
        "digits = datasets.load_digits()\n",
        "data = digits.data.astype(\"float\")\n",
        "data = (data - data.min()) / (data.max() - data.min())\n",
        "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
        "\tdata.shape[1]))\n",
        "\n",
        "# construct the training and testing splits\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "\tdigits.target, test_size=0.25)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
        "print(\"[INFO] {}\".format(nn))\n",
        "nn.fit(trainX, trainY, epochs=1000)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = nn.predict(testX)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(testY.argmax(axis=1), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRF1iVRKRayW",
        "outputId": "ad8cfcda-f7bc-4b63-b22b-4fb37fc4b4c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading MNIST (sample) dataset...\n",
            "[INFO] samples: 1797, dim: 64\n",
            "[INFO] training network...\n",
            "[INFO] NeuralNetwork: 64-32-16-10\n",
            "[INFO] epoch=1, loss=605.1392738\n",
            "[INFO] epoch=100, loss=6.5362080\n",
            "[INFO] epoch=200, loss=3.0182896\n",
            "[INFO] epoch=300, loss=1.9614298\n",
            "[INFO] epoch=400, loss=1.4029129\n",
            "[INFO] epoch=500, loss=1.2850185\n",
            "[INFO] epoch=600, loss=1.2205092\n",
            "[INFO] epoch=700, loss=1.1793839\n",
            "[INFO] epoch=800, loss=1.1508442\n",
            "[INFO] epoch=900, loss=1.1298927\n",
            "[INFO] epoch=1000, loss=1.1138747\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        40\n",
            "           1       0.98      0.95      0.96        42\n",
            "           2       0.98      0.98      0.98        48\n",
            "           3       0.98      0.96      0.97        55\n",
            "           4       0.96      0.98      0.97        45\n",
            "           5       0.98      0.95      0.96        56\n",
            "           6       0.97      0.95      0.96        38\n",
            "           7       0.98      0.95      0.97        44\n",
            "           8       0.96      0.98      0.97        44\n",
            "           9       0.90      0.95      0.92        38\n",
            "\n",
            "    accuracy                           0.96       450\n",
            "   macro avg       0.96      0.96      0.96       450\n",
            "weighted avg       0.97      0.96      0.96       450\n",
            "\n"
          ]
        }
      ]
    }
  ]
}