{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRWj2xAkLwLkFpdcq98CQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novojitdas/Computer-Vision/blob/main/colab/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network"
      ],
      "metadata": {
        "id": "uvSSCU4ZM--E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![nn](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/perceptron-768x559.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "##Activation Functions\n",
        "\n",
        " Activation functions allowing it to learn complex patterns and representations from the data. Activation functions determine the output of a neuron or node in a neural network based on its weighted input.\n",
        "\n",
        " **Sigmoid Function (Logistic Activation):** The sigmoid function squashes the input values to a range between 0 and 1. It's often used in binary classification problems, where the output represents probabilities.\n",
        "\n",
        "**Hyperbolic Tangent Function (tanh):** The tanh function is similar to the sigmoid but maps input values to a range between -1 and 1, making it zero-centered. It is often used in hidden layers of neural networks.\n",
        "\n",
        "**Rectified Linear Unit (ReLU):** ReLU is one of the most widely used activation functions. It returns the input for positive values and zero for negative values. It's computationally efficient and helps mitigate the vanishing gradient problem.\n",
        "\n",
        "**Leaky ReLU:** Leaky ReLU is a variation of ReLU that allows a small gradient for negative inputs, which helps mitigate the \"dying ReLU\" problem where neurons can become inactive during training.\n",
        "\n",
        "**Exponential Linear Unit (ELU):** ELU is another variation of ReLU that has a smoother transition for negative values, which can help with training deep networks. It also allows negative values.\n",
        "\n",
        "![img](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/activation_functions-768x585.png?lossy=2&strip=1&webp=1)"
      ],
      "metadata": {
        "id": "HQEpGTxsdk8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perceptron\n",
        "\n",
        "A perceptron is a fundamental building block in neural networks, and it serves as the simplest form of an artificial neuron. It was developed by Frank Rosenblatt in the late 1950s. A perceptron takes a set of inputs, performs a weighted sum of those inputs, and then applies an activation function to produce an output. The output is often binary (0 or 1) or can be a continuous value, depending on the activation function used.\n",
        "\n",
        "![perceptron](https://i.imgur.com/z0cHJoX.png)\n",
        "\n",
        "Here are the key components and characteristics of a perceptron in a neural network:\n",
        "\n",
        "Input: A perceptron receives a set of input values, often denoted as x1, x2, x3, ..., xn. These inputs can represent features, and they can be real numbers or binary values.\n",
        "\n",
        "Weights: Each input is associated with a weight (often denoted as w1, w2, w3, ..., wn). These weights represent the strength of the connection between the inputs and the perceptron.\n",
        "\n",
        "Weighted Sum: The perceptron computes a weighted sum of the inputs and weights. This sum is represented as:\n",
        "\n",
        "Weighted Sum = (w1 * x1) + (w2 * x2) + (w3 * x3) + ... + (wn * xn)\n",
        "\n",
        "Bias: In addition to the weighted sum, a bias term (often denoted as b) is added to the sum. The bias allows the perceptron to account for situations where all inputs are zero, and it helps in controlling the activation threshold of the neuron.\n",
        "\n",
        "Activation Function: The weighted sum plus the bias is then passed through an activation function (sometimes called a transfer function). The activation function determines the output of the perceptron. Common activation functions include the step function (a simple binary decision), the sigmoid function, the hyperbolic tangent (tanh), or the rectified linear unit (ReLU) function, among others.\n",
        "\n",
        "The step function: Output is 1 if the weighted sum + bias is greater than or equal to 0, and 0 otherwise.\n",
        "Sigmoid function: Produces an output in the range (0, 1), making it suitable for binary classification tasks.\n",
        "Hyperbolic tangent (tanh): Produces an output in the range (-1, 1).\n",
        "ReLU function: Output is zero if the weighted sum + bias is less than 0, and it is equal to the weighted sum + bias otherwise.\n",
        "Output: The final output of the perceptron is the result of the activation function. This output is used as an input for subsequent layers in a neural network.\n",
        "\n",
        "Perceptrons are limited in their capabilities and can only model linearly separable functions, which means they can't handle more complex, nonlinear problems. However, by combining multiple perceptrons in layers and using more advanced activation functions, neural networks can approximate a wide range of functions, making them suitable for tasks like image recognition, natural language processing, and many other machine learning and artificial intelligence applications. These more complex neural networks are often referred to as multi-layer perceptrons (MLPs) or feedforward neural networks."
      ],
      "metadata": {
        "id": "xkSykvRfo9f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Packages\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zW0blOKSsHjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Perceptron"
      ],
      "metadata": {
        "id": "Aj_yRMIysfSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "\tdef __init__(self, N, alpha=0.1):\n",
        "\t\t# initialize the weight matrix and store the learning rate\n",
        "\t\tself.W = np.random.randn(N + 1) / np.sqrt(N)\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\tdef step(self, x):\n",
        "\t\t# apply the step function\n",
        "\t\treturn 1 if x > 0 else 0\n",
        "\n",
        "\tdef fit(self, X, y, epochs=10):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# loop over the desired number of epochs\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\t# take the dot product between the input features\n",
        "\t\t\t\t# and the weight matrix, then pass this value\n",
        "\t\t\t\t# through the step function to obtain the prediction\n",
        "\t\t\t\tp = self.step(np.dot(x, self.W))\n",
        "\n",
        "\t\t\t\t# only perform a weight update if our prediction\n",
        "\t\t\t\t# does not match the target\n",
        "\t\t\t\tif p != target:\n",
        "\t\t\t\t\t# determine the error\n",
        "\t\t\t\t\terror = p - target\n",
        "\n",
        "\t\t\t\t\t# update the weight matrix\n",
        "\t\t\t\t\tself.W += -self.alpha * error * x\n",
        "\n",
        "\tdef predict(self, X, addBias=True):\n",
        "\t\t# ensure our input is a matrix\n",
        "\t\tX = np.atleast_2d(X)\n",
        "\n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# take the dot product between the input features and the\n",
        "\t\t# weight matrix, then pass the value through the step\n",
        "\t\t# function\n",
        "\t\treturn self.step(np.dot(X, self.W))"
      ],
      "metadata": {
        "id": "J7jDUSEisZ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron on bitwise datasets"
      ],
      "metadata": {
        "id": "ukhD8gOvsy9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OR\n"
      ],
      "metadata": {
        "id": "yGPQiphjtJ8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the OR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPSQi27EtXf0",
        "outputId": "fc9b6456-c2e0-42d8-c567-1e353ab884f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awvU3instr7m",
        "outputId": "0e17aa49-6312-4a20-be0f-ea573c990446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0\n",
            "[INFO] data=[0 1], ground-truth=1, pred=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=1\n",
            "[INFO] data=[1 1], ground-truth=1, pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AND"
      ],
      "metadata": {
        "id": "tAx21PymtOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the AND dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)\n",
        "\n",
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjfZdSKxtLv8",
        "outputId": "f4fb6880-07d3-4484-dbb6-697a75c3d6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n",
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0\n",
            "[INFO] data=[0 1], ground-truth=0, pred=0\n",
            "[INFO] data=[1 0], ground-truth=0, pred=0\n",
            "[INFO] data=[1 1], ground-truth=1, pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###XOR"
      ],
      "metadata": {
        "id": "nCrF_FiOtQW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# define our perceptron and train it\n",
        "print(\"[INFO] training perceptron...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)\n",
        "\n",
        "# now that our perceptron is trained we can evaluate it\n",
        "print(\"[INFO] testing perceptron...\")\n",
        "\n",
        "# now that our network is trained, loop over the data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = p.predict(x)\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={}\".format(\n",
        "\t\tx, target[0], pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPteJVuJtSE8",
        "outputId": "b5e5f9fa-6063-4c70-b04a-2a22b149576e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training perceptron...\n",
            "[INFO] testing perceptron...\n",
            "[INFO] data=[0 0], ground-truth=0, pred=1\n",
            "[INFO] data=[0 1], ground-truth=1, pred=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=0\n",
            "[INFO] data=[1 1], ground-truth=0, pred=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Backpropagation\n",
        "The backpropagation algorithm consists of two phases:\n",
        "\n",
        "1. **The forward pass:** where our inputs are passed through the network and output predictions obtained (also known as the propagation phase).\n",
        "\n",
        "2. **The backward pass:** where we compute the gradient of the loss function at the final layer (i.e., predictions layer) of the network and use this gradient to recursively apply the chain rule to update the weights in our network (also known as the weight update phase).\n",
        "\n",
        "To obtain perfect classification accuracy on this problem we’ll need a feedforward neural network with at least a single hidden layer, so let’s go ahead and start with a 2−2−1 architecture (Figure 1, top). This is a good start; however, we’re forgetting to include the bias term. There are two ways to include the bias term b in our network. We can either:\n",
        "\n",
        "1. Use a separate variable.\n",
        "2. Treat the bias as a trainable parameter within the weight matrix by inserting a column of 1’s into the feature vectors.\n",
        "\n",
        "![arch](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/forward_pass_setup-768x774.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "Inserting a column of 1’s into our feature vector is done programmatically, but to ensure we understand this point, let’s update our XOR design matrix to explicitly see this taking place (Table 1, right). As you can see, a column of 1’s have been added to our feature vectors. In practice you can insert this column anywhere you like, but we typically place it either as (1) the first entry in the feature vector or (2) the last entry in the feature vector.\n",
        "\n",
        "Since we have changed the size of our input feature vector (normally performed inside neural network implementation itself so that we do not need to explicitly modify our design matrix), that changes our (perceived) network architecture from 2−2−1 to an (internal) 3−3−1 (Figure 1, bottom).\n",
        "\n",
        "We’ll still refer to this network architecture as 2−2−1, but when it comes to implementation, it’s actually 3−3−1 due to the addition of the bias term embedded in the weight matrix.\n",
        "\n",
        "Finally, recall that both our input layer and all hidden layers require a bias term; however, the final output layer does not require a bias. The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer — it is now a trainable parameter within the weight matrix, thus making training more efficient and substantially easier to implement.\n",
        "\n",
        "To see the forward pass in action, we first initialize the weights in our network, as in Figure 2. Notice how each arrow in the weight matrix has a value associated with it — this is the current weight value for a given node and signifies the amount in which a given input is amplified or diminished. This weight value will then be updated during the backpropagation phase.\n",
        "\n",
        "![fig2](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2021/04/forward_pass_init-768x616.png?lossy=2&strip=1&webp=1)\n",
        "\n",
        "Inserting a column of 1’s into our feature vector is done programmatically, but to ensure we understand this point, let’s update our XOR design matrix to explicitly see this taking place (Table 1, right). As you can see, a column of 1’s have been added to our feature vectors. In practice you can insert this column anywhere you like, but we typically place it either as (1) the first entry in the feature vector or (2) the last entry in the feature vector.\n",
        "\n",
        "Since we have changed the size of our input feature vector (normally performed inside neural network implementation itself so that we do not need to explicitly modify our design matrix), that changes our (perceived) network architecture from 2−2−1 to an (internal) 3−3−1 (Figure 1, bottom).\n",
        "\n",
        "We’ll still refer to this network architecture as 2−2−1, but when it comes to implementation, it’s actually 3−3−1 due to the addition of the bias term embedded in the weight matrix.\n",
        "\n",
        "Finally, recall that both our input layer and all hidden layers require a bias term; however, the final output layer does not require a bias. The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer — it is now a trainable parameter within the weight matrix, thus making training more efficient and substantially easier to implement.\n",
        "\n",
        "To see the forward pass in action, we first initialize the weights in our network, as in Figure 2. Notice how each arrow in the weight matrix has a value associated with it — this is the current weight value for a given node and signifies the amount in which a given input is amplified or diminished. This weight value will then be updated during the backpropagation phase.\n",
        "\n",
        "On the far left of Figure 2, we present the feature vector (0, 1, 1) (and target output value 1 to the network). Here we can see that 0, 1, and 1 have been assigned to the three input nodes in the network. To propagate the values through the network and obtain the final classification, we need to take the dot product between the inputs and the weight values, followed by applying an activation function (in this case, the sigmoid function, σ).\n",
        "\n",
        "Let’s compute the inputs to the three nodes in the hidden layers:\n",
        "\n",
        "σ((0×0.351) + (1×1.076) + (1×1.116)) = 0.899\n",
        "σ((0× −0.097) + (1× −0.165) + (1×0.542)) = 0.593\n",
        "σ((0×0.457) + (1× −0.165) + (1× −0.331)) = 0.378\n",
        "Looking at the node values of the hidden layers (Figure 2, middle), we can see the nodes have been updated to reflect our computation.\n",
        "\n",
        "We now have our inputs to the hidden layer nodes. To compute the output prediction, we once again compute the dot product followed by a sigmoid activation:\n",
        "\n",
        "(1) σ((0.899×0.383) + (0.593× −0.327) + (0.378× −0.329)) = 0.506\n",
        "\n",
        "The output of the network is thus 0.506. We can apply a step function to determine if this output is the correct classification or not:\n",
        "\n",
        "![formula](https://b2633864.smushcdn.com/2633864/wp-content/latex/f4d/f4d4c9bedaba059ad269625523526821-ffffff-000000-0.png?size=172x51&lossy=2&strip=1&webp=1)\n",
        "\n",
        "Applying the step function with net = 0.506 we see that our network predicts 1 which is, in fact, the correct class label. However, our network is not very confident in this class label — the predicted value 0.506 is very close to the threshold of the step. Ideally, this prediction should be closer to 0.98−0.99, implying that our network has truly learned the underlying pattern in the dataset. In order for our network to actually “learn,” we need to apply the backward pass."
      ],
      "metadata": {
        "id": "09-GbVvoNIxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Impementing Backpropagation"
      ],
      "metadata": {
        "id": "96DivCLjO9qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YykIDTmKPCgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "\tdef __init__(self, layers, alpha=0.1):\n",
        "\t\t# initialize the list of weights matrices, then store the\n",
        "\t\t# network architecture and learning rate\n",
        "\t\tself.W = []\n",
        "\t\tself.layers = layers\n",
        "\t\tself.alpha = alpha\n",
        "\n",
        "\t\t# start looping from the index of the first layer but\n",
        "\t\t# stop before we reach the last two layers\n",
        "\t\tfor i in np.arange(0, len(layers) - 2):\n",
        "\t\t\t# randomly initialize a weight matrix connecting the\n",
        "\t\t\t# number of nodes in each respective layer together,\n",
        "\t\t\t# adding an extra node for the bias\n",
        "\t\t\tw = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "\t\t\tself.W.append(w / np.sqrt(layers[i]))\n",
        "\n",
        "\t\t# the last two layers are a special case where the input\n",
        "\t\t# connections need a bias term but the output does not\n",
        "\t\tw = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "\t\tself.W.append(w / np.sqrt(layers[-2]))\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\t# construct and return a string that represents the network\n",
        "\t\t# architecture\n",
        "\t\treturn \"NeuralNetwork: {}\".format(\n",
        "\t\t\t\"-\".join(str(l) for l in self.layers))\n",
        "\n",
        "\tdef sigmoid(self, x):\n",
        "\t\t# compute and return the sigmoid activation value for a\n",
        "\t\t# given input value\n",
        "\t\treturn 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "\tdef sigmoid_deriv(self, x):\n",
        "\t\t# compute the derivative of the sigmoid function ASSUMING\n",
        "\t\t# that `x` has already been passed through the `sigmoid`\n",
        "\t\t# function\n",
        "\t\treturn x * (1 - x)\n",
        "\n",
        "\tdef fit(self, X, y, epochs=1000, displayUpdate=100):\n",
        "\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t# matrix -- this little trick allows us to treat the bias\n",
        "\t\t# as a trainable parameter within the weight matrix\n",
        "\t\tX = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "\t\t# loop over the desired number of epochs\n",
        "\t\tfor epoch in np.arange(0, epochs):\n",
        "\t\t\t# loop over each individual data point and train\n",
        "\t\t\t# our network on it\n",
        "\t\t\tfor (x, target) in zip(X, y):\n",
        "\t\t\t\tself.fit_partial(x, target)\n",
        "\n",
        "\t\t\t# check to see if we should display a training update\n",
        "\t\t\tif epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
        "\t\t\t\tloss = self.calculate_loss(X, y)\n",
        "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
        "\t\t\t\t\tepoch + 1, loss))\n",
        "\n",
        "\tdef fit_partial(self, x, y):\n",
        "\t\t# construct our list of output activations for each layer\n",
        "\t\t# as our data point flows through the network; the first\n",
        "\t\t# activation is a special case -- it's just the input\n",
        "\t\t# feature vector itself\n",
        "\t\tA = [np.atleast_2d(x)]\n",
        "\n",
        "\t\t# FEEDFORWARD:\n",
        "\t\t# loop over the layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# feedforward the activation at the current layer by\n",
        "\t\t\t# taking the dot product between the activation and\n",
        "\t\t\t# the weight matrix -- this is called the \"net input\"\n",
        "\t\t\t# to the current layer\n",
        "\t\t\tnet = A[layer].dot(self.W[layer])\n",
        "\n",
        "\t\t\t# computing the \"net output\" is simply applying our\n",
        "\t\t\t# non-linear activation function to the net input\n",
        "\t\t\tout = self.sigmoid(net)\n",
        "\n",
        "\t\t\t# once we have the net output, add it to our list of\n",
        "\t\t\t# activations\n",
        "\t\t\tA.append(out)\n",
        "\n",
        "\t\t# BACKPROPAGATION\n",
        "\t\t# the first phase of backpropagation is to compute the\n",
        "\t\t# difference between our *prediction* (the final output\n",
        "\t\t# activation in the activations list) and the true target\n",
        "\t\t# value\n",
        "\t\terror = A[-1] - y\n",
        "\n",
        "\t\t# from here, we need to apply the chain rule and build our\n",
        "\t\t# list of deltas `D`; the first entry in the deltas is\n",
        "\t\t# simply the error of the output layer times the derivative\n",
        "\t\t# of our activation function for the output value\n",
        "\t\tD = [error * self.sigmoid_deriv(A[-1])]\n",
        "\n",
        "\t\t# once you understand the chain rule it becomes super easy\n",
        "\t\t# to implement with a `for` loop -- simply loop over the\n",
        "\t\t# layers in reverse order (ignoring the last two since we\n",
        "\t\t# already have taken them into account)\n",
        "\t\tfor layer in np.arange(len(A) - 2, 0, -1):\n",
        "\t\t\t# the delta for the current layer is equal to the delta\n",
        "\t\t\t# of the *previous layer* dotted with the weight matrix\n",
        "\t\t\t# of the current layer, followed by multiplying the delta\n",
        "\t\t\t# by the derivative of the non-linear activation function\n",
        "\t\t\t# for the activations of the current layer\n",
        "\t\t\tdelta = D[-1].dot(self.W[layer].T)\n",
        "\t\t\tdelta = delta * self.sigmoid_deriv(A[layer])\n",
        "\t\t\tD.append(delta)\n",
        "\n",
        "\t\t# since we looped over our layers in reverse order we need to\n",
        "\t\t# reverse the deltas\n",
        "\t\tD = D[::-1]\n",
        "\n",
        "\t\t# WEIGHT UPDATE PHASE\n",
        "\t\t# loop over the layers\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# update our weights by taking the dot product of the layer\n",
        "\t\t\t# activations with their respective deltas, then multiplying\n",
        "\t\t\t# this value by some small learning rate and adding to our\n",
        "\t\t\t# weight matrix -- this is where the actual \"learning\" takes\n",
        "\t\t\t# place\n",
        "\t\t\tself.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "\n",
        "\tdef predict(self, X, addBias=True):\n",
        "\t\t# initialize the output prediction as the input features -- this\n",
        "\t\t# value will be (forward) propagated through the network to\n",
        "\t\t# obtain the final prediction\n",
        "\t\tp = np.atleast_2d(X)\n",
        "\n",
        "\t\t# check to see if the bias column should be added\n",
        "\t\tif addBias:\n",
        "\t\t\t# insert a column of 1's as the last entry in the feature\n",
        "\t\t\t# matrix (bias)\n",
        "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
        "\n",
        "\t\t# loop over our layers in the network\n",
        "\t\tfor layer in np.arange(0, len(self.W)):\n",
        "\t\t\t# computing the output prediction is as simple as taking\n",
        "\t\t\t# the dot product between the current activation value `p`\n",
        "\t\t\t# and the weight matrix associated with the current layer,\n",
        "\t\t\t# then passing this value through a non-linear activation\n",
        "\t\t\t# function\n",
        "\t\t\tp = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "\n",
        "\t\t# return the predicted value\n",
        "\t\treturn p\n",
        "\n",
        "\tdef calculate_loss(self, X, targets):\n",
        "\t\t# make predictions for the input data points then compute\n",
        "\t\t# the loss\n",
        "\t\ttargets = np.atleast_2d(targets)\n",
        "\t\tpredictions = self.predict(X, addBias=False)\n",
        "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\n",
        "\t\t# return the loss\n",
        "\t\treturn loss"
      ],
      "metadata": {
        "id": "R8BJ13wyPJ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backpropagation with bitwise xor"
      ],
      "metadata": {
        "id": "-iVHVQkQPZjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# define our 2-2-1 neural network and train it\n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000)\n",
        "\n",
        "# now that our network is trained, loop over the XOR data points\n",
        "for (x, target) in zip(X, y):\n",
        "\t# make a prediction on the data point and display the result\n",
        "\t# to our console\n",
        "\tpred = nn.predict(x)[0][0]\n",
        "\tstep = 1 if pred > 0.5 else 0\n",
        "\tprint(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(\n",
        "\t\tx, target[0], pred, step))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDFb_KKjPeG4",
        "outputId": "72ac68d9-10c2-4df7-b635-9a397d97d838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch=1, loss=0.5011958\n",
            "[INFO] epoch=100, loss=0.4993236\n",
            "[INFO] epoch=200, loss=0.4986790\n",
            "[INFO] epoch=300, loss=0.4973990\n",
            "[INFO] epoch=400, loss=0.4942430\n",
            "[INFO] epoch=500, loss=0.4852055\n",
            "[INFO] epoch=600, loss=0.4609070\n",
            "[INFO] epoch=700, loss=0.4127845\n",
            "[INFO] epoch=800, loss=0.3217109\n",
            "[INFO] epoch=900, loss=0.1589114\n",
            "[INFO] epoch=1000, loss=0.0645025\n",
            "[INFO] epoch=1100, loss=0.0337388\n",
            "[INFO] epoch=1200, loss=0.0213986\n",
            "[INFO] epoch=1300, loss=0.0152215\n",
            "[INFO] epoch=1400, loss=0.0116346\n",
            "[INFO] epoch=1500, loss=0.0093325\n",
            "[INFO] epoch=1600, loss=0.0077466\n",
            "[INFO] epoch=1700, loss=0.0065959\n",
            "[INFO] epoch=1800, loss=0.0057269\n",
            "[INFO] epoch=1900, loss=0.0050498\n",
            "[INFO] epoch=2000, loss=0.0045088\n",
            "[INFO] epoch=2100, loss=0.0040675\n",
            "[INFO] epoch=2200, loss=0.0037012\n",
            "[INFO] epoch=2300, loss=0.0033928\n",
            "[INFO] epoch=2400, loss=0.0031297\n",
            "[INFO] epoch=2500, loss=0.0029028\n",
            "[INFO] epoch=2600, loss=0.0027053\n",
            "[INFO] epoch=2700, loss=0.0025320\n",
            "[INFO] epoch=2800, loss=0.0023787\n",
            "[INFO] epoch=2900, loss=0.0022422\n",
            "[INFO] epoch=3000, loss=0.0021200\n",
            "[INFO] epoch=3100, loss=0.0020100\n",
            "[INFO] epoch=3200, loss=0.0019104\n",
            "[INFO] epoch=3300, loss=0.0018198\n",
            "[INFO] epoch=3400, loss=0.0017372\n",
            "[INFO] epoch=3500, loss=0.0016615\n",
            "[INFO] epoch=3600, loss=0.0015919\n",
            "[INFO] epoch=3700, loss=0.0015277\n",
            "[INFO] epoch=3800, loss=0.0014683\n",
            "[INFO] epoch=3900, loss=0.0014132\n",
            "[INFO] epoch=4000, loss=0.0013620\n",
            "[INFO] epoch=4100, loss=0.0013142\n",
            "[INFO] epoch=4200, loss=0.0012696\n",
            "[INFO] epoch=4300, loss=0.0012278\n",
            "[INFO] epoch=4400, loss=0.0011886\n",
            "[INFO] epoch=4500, loss=0.0011517\n",
            "[INFO] epoch=4600, loss=0.0011170\n",
            "[INFO] epoch=4700, loss=0.0010843\n",
            "[INFO] epoch=4800, loss=0.0010534\n",
            "[INFO] epoch=4900, loss=0.0010241\n",
            "[INFO] epoch=5000, loss=0.0009964\n",
            "[INFO] epoch=5100, loss=0.0009701\n",
            "[INFO] epoch=5200, loss=0.0009451\n",
            "[INFO] epoch=5300, loss=0.0009213\n",
            "[INFO] epoch=5400, loss=0.0008987\n",
            "[INFO] epoch=5500, loss=0.0008771\n",
            "[INFO] epoch=5600, loss=0.0008565\n",
            "[INFO] epoch=5700, loss=0.0008368\n",
            "[INFO] epoch=5800, loss=0.0008180\n",
            "[INFO] epoch=5900, loss=0.0008000\n",
            "[INFO] epoch=6000, loss=0.0007828\n",
            "[INFO] epoch=6100, loss=0.0007662\n",
            "[INFO] epoch=6200, loss=0.0007503\n",
            "[INFO] epoch=6300, loss=0.0007351\n",
            "[INFO] epoch=6400, loss=0.0007204\n",
            "[INFO] epoch=6500, loss=0.0007063\n",
            "[INFO] epoch=6600, loss=0.0006927\n",
            "[INFO] epoch=6700, loss=0.0006797\n",
            "[INFO] epoch=6800, loss=0.0006671\n",
            "[INFO] epoch=6900, loss=0.0006549\n",
            "[INFO] epoch=7000, loss=0.0006432\n",
            "[INFO] epoch=7100, loss=0.0006319\n",
            "[INFO] epoch=7200, loss=0.0006209\n",
            "[INFO] epoch=7300, loss=0.0006103\n",
            "[INFO] epoch=7400, loss=0.0006001\n",
            "[INFO] epoch=7500, loss=0.0005902\n",
            "[INFO] epoch=7600, loss=0.0005806\n",
            "[INFO] epoch=7700, loss=0.0005713\n",
            "[INFO] epoch=7800, loss=0.0005623\n",
            "[INFO] epoch=7900, loss=0.0005536\n",
            "[INFO] epoch=8000, loss=0.0005451\n",
            "[INFO] epoch=8100, loss=0.0005369\n",
            "[INFO] epoch=8200, loss=0.0005289\n",
            "[INFO] epoch=8300, loss=0.0005211\n",
            "[INFO] epoch=8400, loss=0.0005136\n",
            "[INFO] epoch=8500, loss=0.0005063\n",
            "[INFO] epoch=8600, loss=0.0004991\n",
            "[INFO] epoch=8700, loss=0.0004922\n",
            "[INFO] epoch=8800, loss=0.0004855\n",
            "[INFO] epoch=8900, loss=0.0004789\n",
            "[INFO] epoch=9000, loss=0.0004725\n",
            "[INFO] epoch=9100, loss=0.0004663\n",
            "[INFO] epoch=9200, loss=0.0004602\n",
            "[INFO] epoch=9300, loss=0.0004543\n",
            "[INFO] epoch=9400, loss=0.0004485\n",
            "[INFO] epoch=9500, loss=0.0004429\n",
            "[INFO] epoch=9600, loss=0.0004374\n",
            "[INFO] epoch=9700, loss=0.0004320\n",
            "[INFO] epoch=9800, loss=0.0004268\n",
            "[INFO] epoch=9900, loss=0.0004217\n",
            "[INFO] epoch=10000, loss=0.0004167\n",
            "[INFO] epoch=10100, loss=0.0004118\n",
            "[INFO] epoch=10200, loss=0.0004070\n",
            "[INFO] epoch=10300, loss=0.0004024\n",
            "[INFO] epoch=10400, loss=0.0003978\n",
            "[INFO] epoch=10500, loss=0.0003933\n",
            "[INFO] epoch=10600, loss=0.0003890\n",
            "[INFO] epoch=10700, loss=0.0003847\n",
            "[INFO] epoch=10800, loss=0.0003805\n",
            "[INFO] epoch=10900, loss=0.0003765\n",
            "[INFO] epoch=11000, loss=0.0003724\n",
            "[INFO] epoch=11100, loss=0.0003685\n",
            "[INFO] epoch=11200, loss=0.0003647\n",
            "[INFO] epoch=11300, loss=0.0003609\n",
            "[INFO] epoch=11400, loss=0.0003572\n",
            "[INFO] epoch=11500, loss=0.0003536\n",
            "[INFO] epoch=11600, loss=0.0003501\n",
            "[INFO] epoch=11700, loss=0.0003466\n",
            "[INFO] epoch=11800, loss=0.0003432\n",
            "[INFO] epoch=11900, loss=0.0003398\n",
            "[INFO] epoch=12000, loss=0.0003366\n",
            "[INFO] epoch=12100, loss=0.0003333\n",
            "[INFO] epoch=12200, loss=0.0003302\n",
            "[INFO] epoch=12300, loss=0.0003271\n",
            "[INFO] epoch=12400, loss=0.0003240\n",
            "[INFO] epoch=12500, loss=0.0003211\n",
            "[INFO] epoch=12600, loss=0.0003181\n",
            "[INFO] epoch=12700, loss=0.0003152\n",
            "[INFO] epoch=12800, loss=0.0003124\n",
            "[INFO] epoch=12900, loss=0.0003096\n",
            "[INFO] epoch=13000, loss=0.0003069\n",
            "[INFO] epoch=13100, loss=0.0003042\n",
            "[INFO] epoch=13200, loss=0.0003016\n",
            "[INFO] epoch=13300, loss=0.0002990\n",
            "[INFO] epoch=13400, loss=0.0002964\n",
            "[INFO] epoch=13500, loss=0.0002939\n",
            "[INFO] epoch=13600, loss=0.0002914\n",
            "[INFO] epoch=13700, loss=0.0002890\n",
            "[INFO] epoch=13800, loss=0.0002866\n",
            "[INFO] epoch=13900, loss=0.0002843\n",
            "[INFO] epoch=14000, loss=0.0002819\n",
            "[INFO] epoch=14100, loss=0.0002797\n",
            "[INFO] epoch=14200, loss=0.0002774\n",
            "[INFO] epoch=14300, loss=0.0002752\n",
            "[INFO] epoch=14400, loss=0.0002730\n",
            "[INFO] epoch=14500, loss=0.0002709\n",
            "[INFO] epoch=14600, loss=0.0002688\n",
            "[INFO] epoch=14700, loss=0.0002667\n",
            "[INFO] epoch=14800, loss=0.0002647\n",
            "[INFO] epoch=14900, loss=0.0002627\n",
            "[INFO] epoch=15000, loss=0.0002607\n",
            "[INFO] epoch=15100, loss=0.0002587\n",
            "[INFO] epoch=15200, loss=0.0002568\n",
            "[INFO] epoch=15300, loss=0.0002549\n",
            "[INFO] epoch=15400, loss=0.0002530\n",
            "[INFO] epoch=15500, loss=0.0002512\n",
            "[INFO] epoch=15600, loss=0.0002494\n",
            "[INFO] epoch=15700, loss=0.0002476\n",
            "[INFO] epoch=15800, loss=0.0002458\n",
            "[INFO] epoch=15900, loss=0.0002441\n",
            "[INFO] epoch=16000, loss=0.0002424\n",
            "[INFO] epoch=16100, loss=0.0002407\n",
            "[INFO] epoch=16200, loss=0.0002390\n",
            "[INFO] epoch=16300, loss=0.0002373\n",
            "[INFO] epoch=16400, loss=0.0002357\n",
            "[INFO] epoch=16500, loss=0.0002341\n",
            "[INFO] epoch=16600, loss=0.0002325\n",
            "[INFO] epoch=16700, loss=0.0002310\n",
            "[INFO] epoch=16800, loss=0.0002294\n",
            "[INFO] epoch=16900, loss=0.0002279\n",
            "[INFO] epoch=17000, loss=0.0002264\n",
            "[INFO] epoch=17100, loss=0.0002249\n",
            "[INFO] epoch=17200, loss=0.0002235\n",
            "[INFO] epoch=17300, loss=0.0002220\n",
            "[INFO] epoch=17400, loss=0.0002206\n",
            "[INFO] epoch=17500, loss=0.0002192\n",
            "[INFO] epoch=17600, loss=0.0002178\n",
            "[INFO] epoch=17700, loss=0.0002164\n",
            "[INFO] epoch=17800, loss=0.0002151\n",
            "[INFO] epoch=17900, loss=0.0002137\n",
            "[INFO] epoch=18000, loss=0.0002124\n",
            "[INFO] epoch=18100, loss=0.0002111\n",
            "[INFO] epoch=18200, loss=0.0002098\n",
            "[INFO] epoch=18300, loss=0.0002085\n",
            "[INFO] epoch=18400, loss=0.0002073\n",
            "[INFO] epoch=18500, loss=0.0002060\n",
            "[INFO] epoch=18600, loss=0.0002048\n",
            "[INFO] epoch=18700, loss=0.0002036\n",
            "[INFO] epoch=18800, loss=0.0002024\n",
            "[INFO] epoch=18900, loss=0.0002012\n",
            "[INFO] epoch=19000, loss=0.0002000\n",
            "[INFO] epoch=19100, loss=0.0001988\n",
            "[INFO] epoch=19200, loss=0.0001977\n",
            "[INFO] epoch=19300, loss=0.0001965\n",
            "[INFO] epoch=19400, loss=0.0001954\n",
            "[INFO] epoch=19500, loss=0.0001943\n",
            "[INFO] epoch=19600, loss=0.0001932\n",
            "[INFO] epoch=19700, loss=0.0001921\n",
            "[INFO] epoch=19800, loss=0.0001911\n",
            "[INFO] epoch=19900, loss=0.0001900\n",
            "[INFO] epoch=20000, loss=0.0001889\n",
            "[INFO] data=[0 0], ground-truth=0, pred=0.0102, step=0\n",
            "[INFO] data=[0 1], ground-truth=1, pred=0.9908, step=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=0.9894, step=1\n",
            "[INFO] data=[1 1], ground-truth=0, pred=0.0087, step=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MNIST"
      ],
      "metadata": {
        "id": "eOLm9WHHRYzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST dataset and apply min/max scaling to scale the\n",
        "# pixel intensity values to the range [0, 1] (each image is\n",
        "# represented by an 8 x 8 = 64-dim feature vector)\n",
        "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
        "digits = datasets.load_digits()\n",
        "data = digits.data.astype(\"float\")\n",
        "data = (data - data.min()) / (data.max() - data.min())\n",
        "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
        "\tdata.shape[1]))\n",
        "\n",
        "# construct the training and testing splits\n",
        "(trainX, testX, trainY, testY) = train_test_split(data,\n",
        "\tdigits.target, test_size=0.25)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
        "print(\"[INFO] {}\".format(nn))\n",
        "nn.fit(trainX, trainY, epochs=1000)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = nn.predict(testX)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(testY.argmax(axis=1), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRF1iVRKRayW",
        "outputId": "ad8cfcda-f7bc-4b63-b22b-4fb37fc4b4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading MNIST (sample) dataset...\n",
            "[INFO] samples: 1797, dim: 64\n",
            "[INFO] training network...\n",
            "[INFO] NeuralNetwork: 64-32-16-10\n",
            "[INFO] epoch=1, loss=605.1392738\n",
            "[INFO] epoch=100, loss=6.5362080\n",
            "[INFO] epoch=200, loss=3.0182896\n",
            "[INFO] epoch=300, loss=1.9614298\n",
            "[INFO] epoch=400, loss=1.4029129\n",
            "[INFO] epoch=500, loss=1.2850185\n",
            "[INFO] epoch=600, loss=1.2205092\n",
            "[INFO] epoch=700, loss=1.1793839\n",
            "[INFO] epoch=800, loss=1.1508442\n",
            "[INFO] epoch=900, loss=1.1298927\n",
            "[INFO] epoch=1000, loss=1.1138747\n",
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        40\n",
            "           1       0.98      0.95      0.96        42\n",
            "           2       0.98      0.98      0.98        48\n",
            "           3       0.98      0.96      0.97        55\n",
            "           4       0.96      0.98      0.97        45\n",
            "           5       0.98      0.95      0.96        56\n",
            "           6       0.97      0.95      0.96        38\n",
            "           7       0.98      0.95      0.97        44\n",
            "           8       0.96      0.98      0.97        44\n",
            "           9       0.90      0.95      0.92        38\n",
            "\n",
            "    accuracy                           0.96       450\n",
            "   macro avg       0.96      0.96      0.96       450\n",
            "weighted avg       0.97      0.96      0.96       450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing feedforward neural networks with Keras and TensorFlow\n"
      ],
      "metadata": {
        "id": "Dm-Z-4hsf0PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MNIST WITH Keras and tensorflow"
      ],
      "metadata": {
        "id": "N0sduu14g_Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "PCJby2TcgGm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab the MNIST dataset (if this is your first time using this\n",
        "# dataset then the 11MB download may take a minute)\n",
        "print(\"[INFO] accessing MNIST...\")\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "\n",
        "# each image in the MNIST dataset is represented as a 28x28x1\n",
        "# image, but in order to apply a standard neural network we must\n",
        "# first \"flatten\" the image to be simple list of 28x28=784 pixels\n",
        "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
        "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
        "\n",
        "# scale data to the range of [0, 1]\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "# define the 784-256-128-10 architecture using Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
        "model.add(Dense(128, activation=\"sigmoid\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# train the model using SGD\n",
        "print(\"[INFO] training network...\")\n",
        "sgd = SGD(0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
        "\tepochs=100, batch_size=128)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(testX, batch_size=128)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1),\n",
        "\ttarget_names=[str(x) for x in lb.classes_]))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXfK3Dt_gT-k",
        "outputId": "950c8c0c-0271-472a-9ce7-afee301c6862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] accessing MNIST...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "[INFO] training network...\n",
            "Epoch 1/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 2.2788 - accuracy: 0.1985 - val_loss: 2.2435 - val_accuracy: 0.2751\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 2.2098 - accuracy: 0.3731 - val_loss: 2.1692 - val_accuracy: 0.4832\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 2.1232 - accuracy: 0.4952 - val_loss: 2.0638 - val_accuracy: 0.5830\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 1.9972 - accuracy: 0.5606 - val_loss: 1.9100 - val_accuracy: 0.5879\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 1.8222 - accuracy: 0.6013 - val_loss: 1.7101 - val_accuracy: 0.6503\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 1.6139 - accuracy: 0.6504 - val_loss: 1.4961 - val_accuracy: 0.6799\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 6s 14ms/step - loss: 1.4088 - accuracy: 0.6964 - val_loss: 1.3024 - val_accuracy: 0.7231\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.2327 - accuracy: 0.7292 - val_loss: 1.1435 - val_accuracy: 0.7494\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 1.0908 - accuracy: 0.7550 - val_loss: 1.0168 - val_accuracy: 0.7696\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 6s 14ms/step - loss: 0.9780 - accuracy: 0.7736 - val_loss: 0.9175 - val_accuracy: 0.7881\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.8885 - accuracy: 0.7899 - val_loss: 0.8373 - val_accuracy: 0.8045\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.8157 - accuracy: 0.8033 - val_loss: 0.7712 - val_accuracy: 0.8127\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.7557 - accuracy: 0.8152 - val_loss: 0.7160 - val_accuracy: 0.8283\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.7053 - accuracy: 0.8259 - val_loss: 0.6698 - val_accuracy: 0.8349\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.6624 - accuracy: 0.8346 - val_loss: 0.6295 - val_accuracy: 0.8428\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.6258 - accuracy: 0.8423 - val_loss: 0.5957 - val_accuracy: 0.8501\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5942 - accuracy: 0.8486 - val_loss: 0.5656 - val_accuracy: 0.8577\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.5669 - accuracy: 0.8545 - val_loss: 0.5402 - val_accuracy: 0.8613\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5431 - accuracy: 0.8598 - val_loss: 0.5175 - val_accuracy: 0.8677\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5223 - accuracy: 0.8641 - val_loss: 0.4979 - val_accuracy: 0.8719\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.5039 - accuracy: 0.8682 - val_loss: 0.4804 - val_accuracy: 0.8753\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4876 - accuracy: 0.8722 - val_loss: 0.4651 - val_accuracy: 0.8774\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.4732 - accuracy: 0.8748 - val_loss: 0.4516 - val_accuracy: 0.8810\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4603 - accuracy: 0.8781 - val_loss: 0.4395 - val_accuracy: 0.8830\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.4487 - accuracy: 0.8799 - val_loss: 0.4283 - val_accuracy: 0.8860\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.4383 - accuracy: 0.8827 - val_loss: 0.4180 - val_accuracy: 0.8889\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.4288 - accuracy: 0.8842 - val_loss: 0.4093 - val_accuracy: 0.8913\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 8s 18ms/step - loss: 0.4202 - accuracy: 0.8867 - val_loss: 0.4017 - val_accuracy: 0.8931\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.4125 - accuracy: 0.8880 - val_loss: 0.3940 - val_accuracy: 0.8949\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 6s 14ms/step - loss: 0.4053 - accuracy: 0.8894 - val_loss: 0.3878 - val_accuracy: 0.8957\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3988 - accuracy: 0.8906 - val_loss: 0.3816 - val_accuracy: 0.8976\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3928 - accuracy: 0.8917 - val_loss: 0.3758 - val_accuracy: 0.8987\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3872 - accuracy: 0.8932 - val_loss: 0.3707 - val_accuracy: 0.8991\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3821 - accuracy: 0.8943 - val_loss: 0.3661 - val_accuracy: 0.9000\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3773 - accuracy: 0.8950 - val_loss: 0.3610 - val_accuracy: 0.9013\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3729 - accuracy: 0.8964 - val_loss: 0.3575 - val_accuracy: 0.9014\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3687 - accuracy: 0.8976 - val_loss: 0.3535 - val_accuracy: 0.9031\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3647 - accuracy: 0.8981 - val_loss: 0.3496 - val_accuracy: 0.9033\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3612 - accuracy: 0.8984 - val_loss: 0.3462 - val_accuracy: 0.9039\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3577 - accuracy: 0.8995 - val_loss: 0.3433 - val_accuracy: 0.9048\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3544 - accuracy: 0.9002 - val_loss: 0.3399 - val_accuracy: 0.9050\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.3513 - accuracy: 0.9013 - val_loss: 0.3372 - val_accuracy: 0.9053\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3484 - accuracy: 0.9018 - val_loss: 0.3345 - val_accuracy: 0.9063\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.3455 - accuracy: 0.9022 - val_loss: 0.3320 - val_accuracy: 0.9068\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3428 - accuracy: 0.9030 - val_loss: 0.3292 - val_accuracy: 0.9075\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.3403 - accuracy: 0.9032 - val_loss: 0.3270 - val_accuracy: 0.9076\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3379 - accuracy: 0.9041 - val_loss: 0.3243 - val_accuracy: 0.9079\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.3355 - accuracy: 0.9043 - val_loss: 0.3228 - val_accuracy: 0.9083\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.3333 - accuracy: 0.9050 - val_loss: 0.3205 - val_accuracy: 0.9085\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3311 - accuracy: 0.9057 - val_loss: 0.3188 - val_accuracy: 0.9089\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3290 - accuracy: 0.9057 - val_loss: 0.3165 - val_accuracy: 0.9104\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3269 - accuracy: 0.9065 - val_loss: 0.3147 - val_accuracy: 0.9113\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3250 - accuracy: 0.9068 - val_loss: 0.3127 - val_accuracy: 0.9117\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3231 - accuracy: 0.9073 - val_loss: 0.3114 - val_accuracy: 0.9110\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3212 - accuracy: 0.9076 - val_loss: 0.3097 - val_accuracy: 0.9130\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3194 - accuracy: 0.9083 - val_loss: 0.3080 - val_accuracy: 0.9132\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3178 - accuracy: 0.9089 - val_loss: 0.3061 - val_accuracy: 0.9133\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3160 - accuracy: 0.9092 - val_loss: 0.3048 - val_accuracy: 0.9137\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3144 - accuracy: 0.9097 - val_loss: 0.3031 - val_accuracy: 0.9146\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3128 - accuracy: 0.9101 - val_loss: 0.3017 - val_accuracy: 0.9153\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3113 - accuracy: 0.9104 - val_loss: 0.3000 - val_accuracy: 0.9154\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3097 - accuracy: 0.9114 - val_loss: 0.2990 - val_accuracy: 0.9154\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3083 - accuracy: 0.9115 - val_loss: 0.2974 - val_accuracy: 0.9146\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3068 - accuracy: 0.9118 - val_loss: 0.2963 - val_accuracy: 0.9169\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3054 - accuracy: 0.9123 - val_loss: 0.2950 - val_accuracy: 0.9163\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3040 - accuracy: 0.9128 - val_loss: 0.2940 - val_accuracy: 0.9170\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3027 - accuracy: 0.9133 - val_loss: 0.2925 - val_accuracy: 0.9172\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3013 - accuracy: 0.9132 - val_loss: 0.2911 - val_accuracy: 0.9180\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3000 - accuracy: 0.9138 - val_loss: 0.2903 - val_accuracy: 0.9174\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2987 - accuracy: 0.9144 - val_loss: 0.2891 - val_accuracy: 0.9174\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2975 - accuracy: 0.9146 - val_loss: 0.2879 - val_accuracy: 0.9176\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2962 - accuracy: 0.9148 - val_loss: 0.2872 - val_accuracy: 0.9185\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2950 - accuracy: 0.9152 - val_loss: 0.2862 - val_accuracy: 0.9182\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2937 - accuracy: 0.9158 - val_loss: 0.2846 - val_accuracy: 0.9189\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2927 - accuracy: 0.9157 - val_loss: 0.2835 - val_accuracy: 0.9191\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.2915 - accuracy: 0.9161 - val_loss: 0.2826 - val_accuracy: 0.9192\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2904 - accuracy: 0.9165 - val_loss: 0.2819 - val_accuracy: 0.9196\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2892 - accuracy: 0.9165 - val_loss: 0.2807 - val_accuracy: 0.9220\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2881 - accuracy: 0.9168 - val_loss: 0.2793 - val_accuracy: 0.9208\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2871 - accuracy: 0.9171 - val_loss: 0.2790 - val_accuracy: 0.9204\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2860 - accuracy: 0.9173 - val_loss: 0.2776 - val_accuracy: 0.9211\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2849 - accuracy: 0.9178 - val_loss: 0.2770 - val_accuracy: 0.9210\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.2839 - accuracy: 0.9183 - val_loss: 0.2761 - val_accuracy: 0.9219\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2828 - accuracy: 0.9183 - val_loss: 0.2749 - val_accuracy: 0.9226\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2818 - accuracy: 0.9184 - val_loss: 0.2739 - val_accuracy: 0.9224\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2809 - accuracy: 0.9189 - val_loss: 0.2736 - val_accuracy: 0.9229\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2798 - accuracy: 0.9191 - val_loss: 0.2726 - val_accuracy: 0.9235\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2788 - accuracy: 0.9193 - val_loss: 0.2715 - val_accuracy: 0.9236\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2779 - accuracy: 0.9198 - val_loss: 0.2704 - val_accuracy: 0.9231\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2769 - accuracy: 0.9199 - val_loss: 0.2695 - val_accuracy: 0.9237\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2759 - accuracy: 0.9200 - val_loss: 0.2689 - val_accuracy: 0.9231\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2750 - accuracy: 0.9204 - val_loss: 0.2685 - val_accuracy: 0.9242\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2741 - accuracy: 0.9204 - val_loss: 0.2673 - val_accuracy: 0.9242\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2731 - accuracy: 0.9213 - val_loss: 0.2663 - val_accuracy: 0.9239\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2722 - accuracy: 0.9213 - val_loss: 0.2660 - val_accuracy: 0.9245\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2713 - accuracy: 0.9215 - val_loss: 0.2650 - val_accuracy: 0.9248\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2703 - accuracy: 0.9219 - val_loss: 0.2644 - val_accuracy: 0.9255\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2695 - accuracy: 0.9220 - val_loss: 0.2630 - val_accuracy: 0.9255\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2686 - accuracy: 0.9221 - val_loss: 0.2627 - val_accuracy: 0.9252\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.2676 - accuracy: 0.9225 - val_loss: 0.2614 - val_accuracy: 0.9250\n",
            "[INFO] evaluating network...\n",
            "79/79 [==============================] - 0s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       980\n",
            "           1       0.97      0.98      0.97      1135\n",
            "           2       0.93      0.90      0.91      1032\n",
            "           3       0.91      0.91      0.91      1010\n",
            "           4       0.92      0.94      0.93       982\n",
            "           5       0.91      0.87      0.89       892\n",
            "           6       0.93      0.95      0.94       958\n",
            "           7       0.93      0.93      0.93      1028\n",
            "           8       0.90      0.89      0.89       974\n",
            "           9       0.91      0.91      0.91      1009\n",
            "\n",
            "    accuracy                           0.93     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.93      0.92     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CIFAR10"
      ],
      "metadata": {
        "id": "CUEJ0hingtVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the training and testing data, scale it into the range [0, 1],\n",
        "# then reshape the design matrix\n",
        "print(\"[INFO] loading CIFAR-10 data...\")\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "trainX = trainX.astype(\"float\") / 255.0\n",
        "testX = testX.astype(\"float\") / 255.0\n",
        "trainX = trainX.reshape((trainX.shape[0], 3072))\n",
        "testX = testX.reshape((testX.shape[0], 3072))\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "# initialize the label names for the CIFAR-10 dataset\n",
        "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "\t\"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# define the 3072-1024-512-10 architecture using Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072,), activation=\"relu\"))\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# train the model using SGD\n",
        "print(\"[INFO] training network...\")\n",
        "sgd = SGD(0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
        "\tepochs=100, batch_size=32)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 100), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6DCVhufUgssS",
        "outputId": "628e5fdb-20fa-43ba-9270-f2e8da2bb8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading CIFAR-10 data...\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "[INFO] training network...\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 55s 35ms/step - loss: 1.8308 - accuracy: 0.3481 - val_loss: 1.7121 - val_accuracy: 0.3855\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.6497 - accuracy: 0.4181 - val_loss: 1.7524 - val_accuracy: 0.3755\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.5717 - accuracy: 0.4451 - val_loss: 1.5718 - val_accuracy: 0.4451\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.5131 - accuracy: 0.4687 - val_loss: 1.5113 - val_accuracy: 0.4683\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 1.4646 - accuracy: 0.4832 - val_loss: 1.5231 - val_accuracy: 0.4460\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 53s 34ms/step - loss: 1.4228 - accuracy: 0.4998 - val_loss: 1.4647 - val_accuracy: 0.4872\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 1.3877 - accuracy: 0.5116 - val_loss: 1.4568 - val_accuracy: 0.4748\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 1.3533 - accuracy: 0.5229 - val_loss: 1.4463 - val_accuracy: 0.4919\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 1.3239 - accuracy: 0.5351 - val_loss: 1.5671 - val_accuracy: 0.4390\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 1.2942 - accuracy: 0.5466 - val_loss: 1.3926 - val_accuracy: 0.5039\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.2663 - accuracy: 0.5554 - val_loss: 1.3799 - val_accuracy: 0.5078\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: 1.2429 - accuracy: 0.5614 - val_loss: 1.4488 - val_accuracy: 0.4886\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: 1.2171 - accuracy: 0.5732 - val_loss: 1.4248 - val_accuracy: 0.5002\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.1929 - accuracy: 0.5814 - val_loss: 1.5058 - val_accuracy: 0.4725\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 1.1693 - accuracy: 0.5905 - val_loss: 1.3548 - val_accuracy: 0.5203\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.1480 - accuracy: 0.5960 - val_loss: 1.3684 - val_accuracy: 0.5124\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 1.1270 - accuracy: 0.6057 - val_loss: 1.3247 - val_accuracy: 0.5306\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 1.1032 - accuracy: 0.6115 - val_loss: 1.4468 - val_accuracy: 0.5001\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 1.0809 - accuracy: 0.6220 - val_loss: 1.3505 - val_accuracy: 0.5243\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 1.0599 - accuracy: 0.6301 - val_loss: 1.3286 - val_accuracy: 0.5285\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 1.0395 - accuracy: 0.6354 - val_loss: 1.3132 - val_accuracy: 0.5356\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 1.0168 - accuracy: 0.6452 - val_loss: 1.4594 - val_accuracy: 0.4986\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.9944 - accuracy: 0.6541 - val_loss: 1.3325 - val_accuracy: 0.5293\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.9750 - accuracy: 0.6572 - val_loss: 1.3612 - val_accuracy: 0.5198\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.9518 - accuracy: 0.6694 - val_loss: 1.3292 - val_accuracy: 0.5321\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.9316 - accuracy: 0.6768 - val_loss: 1.3214 - val_accuracy: 0.5351\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.9121 - accuracy: 0.6821 - val_loss: 1.3118 - val_accuracy: 0.5422\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.8895 - accuracy: 0.6885 - val_loss: 1.4177 - val_accuracy: 0.5247\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8690 - accuracy: 0.6995 - val_loss: 1.3143 - val_accuracy: 0.5501\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.8468 - accuracy: 0.7050 - val_loss: 1.4429 - val_accuracy: 0.5305\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.8286 - accuracy: 0.7108 - val_loss: 1.4535 - val_accuracy: 0.5190\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.8056 - accuracy: 0.7210 - val_loss: 1.5528 - val_accuracy: 0.4999\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7858 - accuracy: 0.7261 - val_loss: 1.4100 - val_accuracy: 0.5268\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7647 - accuracy: 0.7354 - val_loss: 1.5131 - val_accuracy: 0.5068\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7453 - accuracy: 0.7429 - val_loss: 1.3751 - val_accuracy: 0.5406\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7242 - accuracy: 0.7486 - val_loss: 1.3893 - val_accuracy: 0.5439\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.7047 - accuracy: 0.7581 - val_loss: 1.3904 - val_accuracy: 0.5531\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6791 - accuracy: 0.7655 - val_loss: 1.3754 - val_accuracy: 0.5531\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6644 - accuracy: 0.7723 - val_loss: 1.5290 - val_accuracy: 0.5223\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.6429 - accuracy: 0.7801 - val_loss: 1.4281 - val_accuracy: 0.5442\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.6229 - accuracy: 0.7874 - val_loss: 1.4673 - val_accuracy: 0.5388\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 46s 30ms/step - loss: 0.6038 - accuracy: 0.7937 - val_loss: 1.4568 - val_accuracy: 0.5371\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.5831 - accuracy: 0.8024 - val_loss: 1.4740 - val_accuracy: 0.5542\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.5632 - accuracy: 0.8089 - val_loss: 1.4818 - val_accuracy: 0.5453\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.5460 - accuracy: 0.8134 - val_loss: 1.5013 - val_accuracy: 0.5409\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.5279 - accuracy: 0.8222 - val_loss: 1.9035 - val_accuracy: 0.4766\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.5103 - accuracy: 0.8287 - val_loss: 1.6121 - val_accuracy: 0.5391\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.4892 - accuracy: 0.8359 - val_loss: 1.5555 - val_accuracy: 0.5470\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.4728 - accuracy: 0.8417 - val_loss: 1.9827 - val_accuracy: 0.4712\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.4543 - accuracy: 0.8481 - val_loss: 1.5800 - val_accuracy: 0.5430\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.4334 - accuracy: 0.8570 - val_loss: 1.6271 - val_accuracy: 0.5407\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.4236 - accuracy: 0.8593 - val_loss: 1.6736 - val_accuracy: 0.5218\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.4054 - accuracy: 0.8665 - val_loss: 1.8100 - val_accuracy: 0.5222\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.3882 - accuracy: 0.8724 - val_loss: 1.6602 - val_accuracy: 0.5481\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.3692 - accuracy: 0.8810 - val_loss: 1.6091 - val_accuracy: 0.5512\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.3547 - accuracy: 0.8836 - val_loss: 1.6603 - val_accuracy: 0.5484\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.3402 - accuracy: 0.8891 - val_loss: 1.7576 - val_accuracy: 0.5393\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3266 - accuracy: 0.8954 - val_loss: 1.8125 - val_accuracy: 0.5313\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.3160 - accuracy: 0.8998 - val_loss: 2.0797 - val_accuracy: 0.4752\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.3013 - accuracy: 0.9054 - val_loss: 1.6715 - val_accuracy: 0.5661\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.2829 - accuracy: 0.9106 - val_loss: 1.8033 - val_accuracy: 0.5440\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.2777 - accuracy: 0.9139 - val_loss: 1.7818 - val_accuracy: 0.5456\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.2628 - accuracy: 0.9193 - val_loss: 1.7663 - val_accuracy: 0.5504\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.2467 - accuracy: 0.9254 - val_loss: 1.9136 - val_accuracy: 0.5329\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.2368 - accuracy: 0.9302 - val_loss: 1.7659 - val_accuracy: 0.5585\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.2234 - accuracy: 0.9334 - val_loss: 1.9169 - val_accuracy: 0.5352\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.2147 - accuracy: 0.9358 - val_loss: 1.8369 - val_accuracy: 0.5511\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 48s 30ms/step - loss: 0.2026 - accuracy: 0.9408 - val_loss: 1.8108 - val_accuracy: 0.5614\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1919 - accuracy: 0.9440 - val_loss: 1.8586 - val_accuracy: 0.5641\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 0.1843 - accuracy: 0.9472 - val_loss: 2.0442 - val_accuracy: 0.5566\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.1752 - accuracy: 0.9499 - val_loss: 1.8924 - val_accuracy: 0.5592\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 0.1648 - accuracy: 0.9538 - val_loss: 2.0125 - val_accuracy: 0.5468\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.1512 - accuracy: 0.9591 - val_loss: 1.9820 - val_accuracy: 0.5570\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1403 - accuracy: 0.9634 - val_loss: 1.9713 - val_accuracy: 0.5587\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 46s 29ms/step - loss: 0.1350 - accuracy: 0.9653 - val_loss: 2.1901 - val_accuracy: 0.5253\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 0.1318 - accuracy: 0.9654 - val_loss: 2.1691 - val_accuracy: 0.5354\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 0.1220 - accuracy: 0.9688 - val_loss: 2.0498 - val_accuracy: 0.5548\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 48s 31ms/step - loss: 0.1127 - accuracy: 0.9724 - val_loss: 2.0129 - val_accuracy: 0.5583\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 45s 29ms/step - loss: 0.1089 - accuracy: 0.9730 - val_loss: 2.0624 - val_accuracy: 0.5452\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.1025 - accuracy: 0.9756 - val_loss: 2.1047 - val_accuracy: 0.5407\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 47s 30ms/step - loss: 0.0935 - accuracy: 0.9785 - val_loss: 2.0729 - val_accuracy: 0.5588\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 0.0892 - accuracy: 0.9797 - val_loss: 2.0736 - val_accuracy: 0.5581\n",
            "Epoch 83/100\n",
            " 185/1563 [==>...........................] - ETA: 41s - loss: 0.0797 - accuracy: 0.9819"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-05d375d5f8b9>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n\u001b[1;32m     29\u001b[0m \tmetrics=[\"accuracy\"])\n\u001b[0;32m---> 30\u001b[0;31m H = model.fit(trainX, trainY, validation_data=(testX, testY),\n\u001b[0m\u001b[1;32m     31\u001b[0m \tepochs=100, batch_size=32)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}